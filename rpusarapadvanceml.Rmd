---
title: "advance machine learning"
author: "rajendra"
date: "3/4/2020"
output: html_document
---
```{r}

library(keras)
# Number of words to consider as features
max_features <- 10000
imdb <- dataset_imdb(num_words = max_features)
embedding_layer <- layer_embedding(input_dim = 10000, output_dim = 64) 

# Cut texts after this number of words 
# (among top max_features most common words)
maxlen <-150

# Load the data as lists of integers.
imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb

# This turns our lists of integers
# into a 2D integer tensor of shape `(samples, maxlen)`

x_test <- pad_sequences(x_test, maxlen = maxlen)
x_train <- pad_sequences(x_train, maxlen = maxlen)
set.seed(123)
train_index <- sample(1:nrow(x_train),nrow(x_train)*0.004)

train_data <- x_train[train_index,]
partial_x_train <- x_train[-train_index,]
set.seed(123)
val_indices <- sample(1:nrow(partial_x_train),nrow(partial_x_train)*0.402)
x_val <- partial_x_train[val_indices,]

set.seed(123)

train_label <- y_train[train_index]
partial_y_train <- y_train[-train_label]
set.seed(123)
y_val <- partial_y_train[val_indices]



#using embedding layer
model <- keras_model_sequential() %>% 
  # We specify the maximum input length to our Embedding layer
  # so we can later flatten the embedded inputs
  layer_embedding(input_dim = 10000, output_dim = 8, 
                  input_length = maxlen) %>% 
  # We flatten the 3D tensor of embeddings 
  # into a 2D tensor of shape `(samples, maxlen * 8)`
  layer_flatten() %>% 
  # We add the classifier on top
  layer_dense(units = 1, activation = "sigmoid") 

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  train_data, train_label,
  epochs = 10,
  batch_size = 32,
  validation_split = 0.2
)
############...........###########
word_index <- dataset_imdb_word_index()

glove_dir = '~/'
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt"))

embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
for (i in 1:length(lines)) {
  line <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  word <- values[[1]]
  embeddings_index[[word]] <- as.double(values[-1])
}

cat("Found", length(embeddings_index), "word vectors.\n")

embedding_dim <- 100

embedding_matrix <- array(0, c(10000, embedding_dim))

for (word in names(word_index)) {
  index <- word_index[[word]]
  if (index < 10000) {
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector))
      # Words not found in the embedding index will be all zeros.
      embedding_matrix[index+1,] <- embedding_vector
  }
}

##define a model 
max_words = 10000
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

summary(model)

### Load the GloVe embeddings in the model

get_layer(model, index = 1) %>% 
  set_weights(list(embedding_matrix)) %>% 
  freeze_weights()

### Train and evaluate

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

save_model_weights_hdf5(model, "pre_trained_glove_model.h5")

plot(history)
```


